{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets is installed\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q datasets\n",
    "\n",
    "package = \"datasets\"\n",
    "\n",
    "if pkgutil.find_loader(package) is None:\n",
    "    print(package,\"is not installed in python environment\")\n",
    "else:\n",
    "    print(package,\"is installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerate is a user-friendly tool designed to provide developers with the flexibility of writing their own training Loops for pytorch models while avoiding the hassle of dealing with the extra code required\n",
    "for Multi-Device setups \n",
    "\n",
    "Think of accelerate as your personal assistant taking care of all the tedious parts of your code associated with running your model on different kinds of devices whether there are multiple gpus tpus or even enabling mixed Precision in practice \n",
    "\n",
    "This means that you can add just a handful of lines to your typical pytorch training script and then your model can run smoothly on a variety of setups ranging from a single CPU or GPU to multiple CPUs or  gpus and even tpus\n",
    "\n",
    "Even better you can switch between different environments without modifying your code which is super handy for debugging on your local machine before moving to larger scale training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/pythia-410m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for the bits and bytes quantization Library\n",
    "\n",
    "- Quantization is a technique to compress neural networks by reducing the amount of bits that represent the weights of the model \n",
    "- Here it's configured to load the model in a 4-bit representation using the normal float\n",
    "optimized quantization type and the B float 16 data type for computation\n",
    "- Double quantization is also enabled - a technique to further reduce the storage requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal language modeling is a type of language modeling task where the model generates the next token in a sequence based on the pre previous tokens \n",
    "\n",
    "- It's called causal because the model can't see future tokens when predicting current token \n",
    "- It's as if the model is reading the text from left to right just like a human \n",
    "- This kind of model is frequently used for text generation \n",
    "- GPT2 is a well-known example of a causal language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map={\"\":0})\n",
    "\n",
    "\n",
    "# device_count = torch.cuda.device_count()\n",
    "# if device_count > 0:\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the model - Apply PEFT (QLoRA)\n",
    "\n",
    "We have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PEFT (parameter efficient fine tuning). \n",
    "- It involves a new wave of machine learning that allow us to tweak pre-trained language models for different applications without adjusting all the model parameters \n",
    "- This approach is handy because fine-tuning large llms can be extremely resource intensive \n",
    "- Therefore by fine-tuning only a fraction of the model's parameters PEFT methods significantly reduce the computational costs and storage costs \n",
    "- Some of the popular peft methods include Lora, prefix tuning, p-tuning, adalora and QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantization is a process that involves reducing the amount of data required to represent an input \n",
    "- This is typically achieved by converting a data type with a high bit count into a data type with a low bit count \n",
    "- For instance a 32-bit floating Point number might be converted into an 8-bit integer\n",
    "- This process helps manage the range of the lower bit data type more effectively by rescaling the input data type into the target data types range using normalization \n",
    "- However a problem that arises with this technique is that it can be skewed by outliers or large magnitude values in the input data \n",
    "- These outliers can cause the quantization bins which are specific big combinations to be used inefficiently \n",
    "- This can lead to some bins being populated with few or no numbers essentially wasting computational resources \n",
    "- To address this issue a common approach is to split the input tensor into several smaller chunks or blocks which are independently quantized \n",
    "- Each of these blocks would have its own quantization constant ensuring a more efficient usage of the quantization bins \n",
    "- This method known as blockwise k-bit quantization is more resistant to outliers and can lead to more efficient computations and better performance when processing large data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By looking at the number of trainable parameters you can see how many parameters we're actually training \n",
    "- Since the goal is to achieve parameter efficient fine tuning, you should expect to see fewer trainable parameters in the Lora model in comparison to the original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA config allows you to control how LoRA is applied to the base model by manipulating the hyper\n",
    "parameters\n",
    "\n",
    "r - is the rank of the update matrices expressed in integers \n",
    "A lower number for this would represent a smaller update Matrix with fewer trainable parameters\n",
    "\n",
    "alpha - LoRA scaling parameter \n",
    "\n",
    "r and alpha together control the total number of final trainable parameters when using Lora giving you the flexibility to balance and performance with compute efficiency \n",
    "\n",
    "If you have more parameters you're going to have a better performing model but if you have less parameters you're going to be much more computationally efficient\n",
    "\n",
    "target module - specifies the modules for example, attention blocks to apply to the Lora update matrices \n",
    "\n",
    "In this case we're targeting query key and value but it's possible to Target just the query in the key or some other combination\n",
    "\n",
    "dropout - likelihood of co-adaptation where multiple neurons extract identical or very similar features from the input data. this phenomenon can occur when different neurons share nearly identical connection weights and this co-adaptation not only wastes computational resources but can also lead to overfitting \n",
    "\n",
    "To address this we use dropout in the n/w during training. dropout involves randomly disabling a fraction of neurons in a layer at each training step by setting their values to zero. this fraction is often referred to as the dropout rate\n",
    "\n",
    "bias - specifies if the bias parameter should be trained this can be none, all or LoRA only \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do the get peft model, we're wrapping the base model so that the update matrices which are the low rank decomposition matrices of Lora are added to their respective places \n",
    "\n",
    "So this is actually the part where we inject the parameters that we're going to be tuning into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786432 || all params: 255125504 || trainable%: 0.3082529922214284\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# total parameters- 255 M "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset\n",
    "\n",
    "Let's load a common dataset, english quotes, to fine tune our model on famous quotes.\n",
    "\n",
    "In the map function we feed the data through our tokenizer in order to get the machine readable tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"kotzeje/lamini_docs.jsonl\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = load_dataset(\"kotzeje/lamini_docs.jsonl\")\n",
    "\n",
    "# data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I evaluate the performance and quality of the generated text from Lamini models?',\n",
       " \"Can I find information about the code's approach to handling long-running tasks and background jobs?\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question'][0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\",\n",
       " 'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['answer'][:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is responsible for converting your input text into a format that the model can understand typically a sequence of integer token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(text,\n",
    "                                 return_tensors=\"np\",\n",
    "                                 padding=True)\n",
    "\n",
    "    max_length = min(tokenized_inputs[\"input_ids\"].shape[1], 2048)\n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text,\n",
    "                                 return_tensors=\"np\",\n",
    "                                 truncation=True,\n",
    "                                 max_length=max_length)\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f65a2b680447d29f5997b5018e15d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data.map(tokenize_function,\n",
    "                             batched=True,\n",
    "                             batch_size=1,\n",
    "                             drop_last_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1120\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 280\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True, seed=123)\n",
    "\n",
    "print(split_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training\n",
    "\n",
    "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the part where we actually train the QLora parameters. Here we are defining the hyper-parameters in the training arguments. You can change the values of most of the parameters, however if you prefer the only hyper parameter that's really required is the output directory which specifies where to save your model at the end of each epoch \n",
    "the trainer will evaluate the metric and then save the training checkpoint \n",
    "- Note that this is just a walk through but if you want the best performing model you can even perform hyper parameter tuning to achieve optimal results. choosing the right hyper parameters can significantly affect the performance of the model.\n",
    "- Also note that we're using the paged atomw 8-bit Optimizer. This is taking advantage of the paged optimizer concept that was also introduced in that Q Laura paper. This tool behaves as a mechanism to control the memory traffic. In instances where GPU is reaching memory capacity during data training, this feature intervenes. It automatically transfers data between the GPU and the CPU effectively averting memory related issues. This mechanism resembles the process in which a computer shuffles data between RAM and its disk storage when facing low memory scenarios. Paged optimizers harness this feature. So when GPU memory reaches its limit optimizer states are temporarily relocated to the CPU Ram freeing up space on the GPU. These states are then reloaded back onto the GPU memory as needed in the optimizer update step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's break down some of these hyper parameters \n",
    "- The per device train batch size - batch size for training\n",
    "- Gradient accumulation steps is the number of steps to accumulate the gradients before performing an update\n",
    "- The warm-up steps is the number of steps for learning rate warm-up \n",
    "- The max steps is the total number of steps for training \n",
    "- fp16 equals true is whether to use the 16-bit half Precision floating Point numbers for the training as opposed to the default 32-bit ones\n",
    "- Logging steps is the number of steps between each logging event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "epochs = 3\n",
    "trained_model_name = f\"/home/raghu/DL/topics/LLM/Finetune-LLM/outputs/lamini_docs_{epochs}_epochs\"\n",
    "output_dir = trained_model_name\n",
    "\n",
    "training_args = transformers.TrainingArguments(per_device_train_batch_size=8,\n",
    "                                               gradient_accumulation_steps=4,\n",
    "                                               warmup_steps=2,\n",
    "                                               evaluation_strategy=\"epoch\",\n",
    "                                               learning_rate=1.0e-5,\n",
    "                                               fp16=True,\n",
    "                                               #logging_steps=1,\n",
    "                                               output_dir=output_dir,\n",
    "                                               num_train_epochs=epochs,\n",
    "                                               optim=\"paged_adamw_8bit\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we set model.config.use_cache = False\n",
    "\n",
    "This line disables caching in the model configuration\n",
    "\n",
    "Caching can speed up training by storing the model's past computations but it may produce warnings in some cases so it's being disabled here but it's recommended to enable it again when you're doing inferencing\n",
    "\n",
    "The last line here trainer.train is the command that starts the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be81db1d7cd041ce83981b2ce5e3521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9c6d8803134b80bdca9b5ecaebe6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5501582622528076, 'eval_runtime': 3.8888, 'eval_samples_per_second': 72.002, 'eval_steps_per_second': 9.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ae4a6e20134a6ab039f2a034bcc189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3736860752105713, 'eval_runtime': 3.9372, 'eval_samples_per_second': 71.116, 'eval_steps_per_second': 8.889, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc2b9b6d92f439eb0466f11cd83bb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3215787410736084, 'eval_runtime': 3.9323, 'eval_samples_per_second': 71.206, 'eval_steps_per_second': 8.901, 'epoch': 3.0}\n",
      "{'train_runtime': 159.8752, 'train_samples_per_second': 21.016, 'train_steps_per_second': 0.657, 'train_loss': 2.464066569010417, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105, training_loss=2.464066569010417, metrics={'train_runtime': 159.8752, 'train_samples_per_second': 21.016, 'train_steps_per_second': 0.657, 'train_loss': 2.464066569010417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the save pre-trained method of the Lora model to save the Model and inference Laura only parameters locally\n",
    "\n",
    "Alternatively you can use the push to HUB method to upload these parameters directly to the hugging face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of distributed/parallel training\n",
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "model_to_save.save_pretrained(\"/home/raghu/DL/topics/LLM/Finetune-LLM/outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Save the Model ===============\n",
    "# save_dir = f'{output_dir}/final'\n",
    "# trainer.save_model(save_dir)\n",
    "# print(\"Saved model to:\", save_dir)\n",
    "\n",
    "\n",
    "# # ===== Load and test the model =======\n",
    "# finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "# finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('/home/raghu/DL/topics/LLM/Finetune-LLM/outputs')\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "To do inference, you can take some text, tokenize it, put it onto the GPU then feed it through the LLM to get the machine readable outputs\n",
    "\n",
    "Then just convert those machine readable outputs to human readable outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "def inference(text, \n",
    "              model, \n",
    "              tokenizer, \n",
    "              max_input_tokens=1000, \n",
    "              max_output_tokens=100, \n",
    "              temperature=1.0):\n",
    "\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_output_tokens,\n",
    "        temperature = temperature,\n",
    "        do_sample=True,\n",
    "        top_p = 0.95\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, \n",
    "                                                        skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Question input (test)****: Is it possible to fine-tune Lamini on a specific dataset for text generation in legal documents?\n",
      "****Finetuned slightly model's answer****: \n",
      "\n",
      "\n",
      "I am trying to write code to detect if the code section of a document is using legal terminology. I am trying to do this in the context of a specific case and I don't know how to proceed to this. I will be using Lamini-Text and Lamini-Rng. I know that this is possible with Rng but I am not sure if\n"
     ]
    }
   ],
   "source": [
    "test_question = split_dataset[\"test\"][0]['question']\n",
    "print(\"****Question input (test)****:\", test_question)\n",
    "\n",
    "print(\"****Finetuned slightly model's answer****: \")\n",
    "print(inference(test_question, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Finetuned slightly model's answer*****: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I'm just not sure what you're seeing, from the JFSE.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"****Finetuned slightly model's answer*****: \")\n",
    "print(inference(\"How can I evaluate the performance and quality of the generated text??\", \n",
    "                model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned slightly model's answer: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I've found the answer to my query problem.\n",
      "This was a problem with this error.\n",
      "In this line I was trying to type \"for\" with \"for\" that would not work for me because my code (Python 3.5) didn't match.\n",
      "I changed the code as below.\n",
      "\n",
      "I changed the code, and it works, it is just that I did not like it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(\"Tell me about the Keras API?\", model, tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
